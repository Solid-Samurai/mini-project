{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124219df-676e-4536-9122-5c58c860ba0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\tamil\\python\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\tamil\\python\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\tamil\\python\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tamil\\python\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\tamil\\python\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\tamil\\python\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tamil\\python\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tamil\\python\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tamil\\python\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tamil\\python\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tamil\\python\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tamil\\python\\lib\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tamil\\python\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tamil\\python\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827df5ad-58b2-4ed8-88ce-c6b201fd7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89a64a4-79fe-4aa2-a4d0-d0cf44cdf677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dataset Class for Water Meter Digits\n",
    "class WaterMeterDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for label in range(10):  # Assuming folder structure: image_folder/0, image_folder/1, ..., image_folder/9\n",
    "            digit_folder = os.path.join(image_folder, str(label))\n",
    "            if not os.path.exists(digit_folder):\n",
    "                continue\n",
    "            for img_name in os.listdir(digit_folder):\n",
    "                img_path = os.path.join(digit_folder, img_name)\n",
    "                self.images.append(img_path)\n",
    "                self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbfe447-7a74-4b25-b8ed-b9a69c6bb836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "287f4294-8808-4205-b88c-d1a4aad14e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load Dataset\n",
    "train_dataset = WaterMeterDataset('digits updated', transform=transform)\n",
    "val_dataset = WaterMeterDataset('digits_jpeg', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38abac56-9fd1-41ee-affa-41167d512f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load Pretrained ResNet-18\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(512, 10)  # Replace final layer for digit classification\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "781cc1a3-885d-49d8-ae20-63a7d8f00835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a62d8c5-787f-4eac-98f8-390a9d74db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Training Loop\n",
    "def train_model(model, train_loader, val_loader, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "        validate_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f9be1c5-a191-48aa-84b3-687ba5af6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Validation\n",
    "def validate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Validation Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "273afd76-e8e1-438c-be9d-c2fa43ba7c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.07409259209112094\n",
      "Validation Accuracy: 98.73%\n",
      "Epoch 2, Loss: 0.01739268904172873\n",
      "Validation Accuracy: 100.00%\n",
      "Epoch 3, Loss: 0.01479630269252839\n",
      "Validation Accuracy: 96.07%\n",
      "Epoch 4, Loss: 0.012295131358528897\n",
      "Validation Accuracy: 100.00%\n",
      "Epoch 5, Loss: 0.01884148750573989\n",
      "Validation Accuracy: 99.93%\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "280ff82d-7aa8-4c45-91d1-da57ca5e51e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Save Model\n",
    "torch.save(model.state_dict(), 'water_meter_resnet18.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0b6c69d-da13-4f4f-83a2-b3b990850e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Load and Test on a New Image\n",
    "def predict_digit(model, image_path):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    \n",
    "    return predicted.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42f4d58a-3ad8-418f-ab41-eeed7a5a8b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit: 7\n"
     ]
    }
   ],
   "source": [
    "# Test on a new image\n",
    "print(\"Predicted Digit:\", predict_digit(model, 'Untitled Folder/image/id_1_value_13_116.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd37c99-528b-4df3-a394-edf32f60bed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
